{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possíveis ideias para melhoras:\n",
    " - Testar modelos diferentes\n",
    " - Testar modelos com subsets das variáveis\n",
    " - Testar outros tipos de ensemble\n",
    " - Testar mais hiperparâmetros nos modelos usados\n",
    " - Testar os modelos com as saídas das camadas de embedding da rede neural\n",
    " - Fazer mais feature engineering\n",
    "\n",
    "Avanços:\n",
    " - Fazer uma submissão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# definindo o nome do arquivo reduzido de treino\n",
    "filename_train_reduced  = r'data\\reduced_training_set.csv'\n",
    "\n",
    "# definindo os nomes dos arquivos chunkificados\n",
    "filename_train_new = r'data\\chunks\\train.csv'\n",
    "filename_test_new = r'data\\chunks\\test.csv'\n",
    "if not os.path.isdir(os.path.dirname(filename_test_new)):\n",
    "    os.makedirs(os.path.dirname(filename_test_new))\n",
    "\n",
    "\n",
    "# definindo se leremos o arquivo de treino inteiro ou apenas uma parcela dele\n",
    "entire_file = False\n",
    "\n",
    "# definindo se rodaremos a função que diminue o arquivo\n",
    "run_function = 0\n",
    "\n",
    "# proporção de linhas que entram no dataset reduzido\n",
    "proportion = 0.1\n",
    "\n",
    "# definindo a coluna y\n",
    "y_column = 'is_attributed'\n",
    "\n",
    "# definindo se vamos olhar apenas para o arquivo sample\n",
    "sample_only = True\n",
    "\n",
    "# chunkified\n",
    "chunkified = False\n",
    "\n",
    "# Se vamos usar a coluna ip\n",
    "use_ip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [
     3,
     14,
     23,
     43,
     83,
     116
    ]
   },
   "outputs": [],
   "source": [
    "# definição de funções\n",
    "\n",
    "# função que reduz o dataset\n",
    "def red_csv(filename_original, filename_new, proportion):\n",
    "    with open(filename_original, 'r') as fo:\n",
    "        with open(filename_new, 'w') as fn:\n",
    "            csv_reader = csv.reader(fo)\n",
    "            csv_writer = csv.writer(fn)\n",
    "            first_line = next(csv_reader)\n",
    "            csv_writer.writerow(first_line)\n",
    "            for row in tqdm(csv_reader):\n",
    "                if np.random.binomial(1, proportion):\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "def red_csv2(filename_original, filename_new, proportion):\n",
    "    columns = pd.read_csv(filename_original, nrows=1)\n",
    "    with open(filename_new, 'w') as fn:\n",
    "        csv_writer = csv.writer(fn)\n",
    "        csv_writer.writerow(list(columns))\n",
    "        for chunk in tqdm(pd.read_csv(filename_original, chunksize=10000000)):\n",
    "            s = chunk.sample(int(chunk.shape[0]*proportion))\n",
    "            csv_writer.writerows(s.values.tolist())\n",
    "            \n",
    "def split_train_in_files(filename_original, filename_train_new, filename_test_new, proportion):\n",
    "    columns = pd.read_csv(filename_original, nrows=1)\n",
    "    with open(filename_train_new, 'w') as ftn:\n",
    "        with open(filename_test_new, 'w') as ftestn:\n",
    "            csv_writer_train = csv.writer(ftn)\n",
    "            csv_writer_test = csv.writer(ftestn)\n",
    "            \n",
    "            csv_writer_train.writerow(list(columns))\n",
    "            csv_writer_test.writerow(list(columns))\n",
    "            \n",
    "            for chunk in tqdm(pd.read_csv(filename_original, chunksize=10000000)):\n",
    "                sample_test = chunk.sample(int(chunk.shape[0]*proportion))\n",
    "                sample_train = chunk[~chunk.index.isin(list(sample_test.index))]\n",
    "                \n",
    "                csv_writer_train.writerows(sample_train.values.tolist())\n",
    "                csv_writer_test.writerows(sample_test.values.tolist())\n",
    "            \n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# função que treina, prediz, e  calcula o roc_auc\n",
    "def train_and_predict(model, X_train = '', y_train = '', X_test = '', y_test = '', prob=False, chunkified = False, partial = False):\n",
    "    if chunkified:\n",
    "        for chunk in tqdm(pd.read_csv(filename_train_new, dtype=type_dict, parse_dates=['click_time'], \n",
    "                                      usecols=usecols, chunksize=1000000)):\n",
    "            extract_time_features(chunk, 'click_time')\n",
    "            X = chunk[[col for col in chunk.columns if col != y_column]].copy()\n",
    "            y = chunk[y_column].copy()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "            if partial:\n",
    "                model.partial_fit(X_train, y_train)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            if prob:\n",
    "                predictions_train = model.predict_proba(X_train)[:,1]\n",
    "                predictions_test = model.predict_proba(X_test)[:,1]\n",
    "            else:\n",
    "                predictions_train = model.predict(X_train)            \n",
    "                predictions_test = model.predict(X_test)\n",
    "            \n",
    "            roc_auc_train = roc_auc_score(y_train, predictions_train)\n",
    "            roc_auc_test = roc_auc_score(y_test, predictions_test)\n",
    "            print('ROC-AUC no treino: ', roc_auc_train)\n",
    "            print('ROC-AUC no teste deste chunk: ', roc_auc_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions_train = model.predict(X_train)\n",
    "    #     conf_matrix_train = confusion_matrix(y_train, predictions_train)\n",
    "    #     report_train = classification_report(y_train, predictions_train)\n",
    "        roc_auc_train = roc_auc_score(y_train, predictions_train)\n",
    "\n",
    "        predictions_test = model.predict(X_test)\n",
    "    #     conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "    #     report_test = classification_report(y_test, predictions_test)\n",
    "        roc_auc_test = roc_auc_score(y_test, predictions_test)\n",
    "    return roc_auc_train, roc_auc_test\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# função que treina um modelo para cada chunk do arquivo; depois podemos fazer uma média dos modelos\n",
    "def ensemble_chunks(model, X_train = '', y_train = '', X_test = '', y_test = '', prob=False):\n",
    "    model_list = []\n",
    "    for chunk in tqdm(pd.read_csv(filename_train_new, dtype=type_dict, parse_dates=['click_time'], \n",
    "                                      usecols=usecols, chunksize=1000000)):\n",
    "            extract_time_features(chunk, 'click_time')\n",
    "            X = chunk[[col for col in chunk.columns if col != y_column]].copy()\n",
    "            y = chunk[y_column].copy()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            if prob:\n",
    "                predictions_train = model.predict_proba(X_train)[:,1]\n",
    "                predictions_test = model.predict_proba(X_test)[:,1]\n",
    "            else:\n",
    "                predictions_train = model.predict(X_train)            \n",
    "                predictions_test = model.predict(X_test)\n",
    "            \n",
    "            roc_auc_train = roc_auc_score(y_train, predictions_train)\n",
    "            roc_auc_test = roc_auc_score(y_test, predictions_test)\n",
    "            print('ROC-AUC no treino: ', roc_auc_train)\n",
    "            print('ROC-AUC no teste deste chunk: ', roc_auc_test)\n",
    "            model_list.append(clone(model))\n",
    "    return model_list\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# classe que calcula o roc_auc como callback do fit do keras\n",
    "# Vamos importar esse callback para tratar o metódo fit.\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# Vamos trazer o roc_auc do sklearn para nos ajudar a calcular a métrica\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class roc_callback(Callback):\n",
    "    \n",
    "    '''\n",
    "        Aqui vamos tratar o final do metodo fit do keras, vamos calcular \n",
    "        o roc_auc_score no final do fit\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self,train_chunk_x,train_chunk_y, cols, test_files=[]):\n",
    "        # Vamos inicializar os dados de entrada do modelo e o rótulo, bem como\n",
    "        # os índices do csv que serão nossa validação\n",
    "        self.x = train_chunk_x\n",
    "        self.y = train_chunk_y\n",
    "        val = pd.concat([pd.read_csv(file, \n",
    "                                          dtype=type_dict, \n",
    "                                          parse_dates=['click_time'], \n",
    "                                          usecols=usecols) for file in test_files],\n",
    "                                   axis=0)\n",
    "        extract_time_features(val, 'click_time')\n",
    "        \n",
    "        # Separamos entrada e dados que queremos prever\n",
    "        self.val_x = [val[col] for col in cols]\n",
    "        self.val_y = val['is_attributed'].values\n",
    "        \n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Calculamos as previsões para o treinamento\n",
    "        y_pred = self.model.predict(self.x, batch_size=16394 * 2)\n",
    "        \n",
    "        # Calculamos o auc de treino\n",
    "        roc_train = roc_auc_score(y_true=self.y, y_score=y_pred)        \n",
    "        \n",
    "        # Calculamos a previsão para o conjunto de validação\n",
    "        y_pred = self.model.predict(self.val_x, batch_size=16394 * 2)\n",
    "        \n",
    "        # Calculamos o auc de validação\n",
    "        roc_test = roc_auc_score(y_true=self.val_y, y_score=y_pred)\n",
    "\n",
    "        # Printamos em um formato bonitinho\n",
    "        print('\\nroc_train: {:.4f}'.format(roc_train))\n",
    "        print('roc_test: {:.4f}'.format(roc_test))\n",
    "        \n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\reduced_training_set.csv', 'data\\\\sample_submission.csv', 'data\\\\test.csv', 'data\\\\train.csv', 'data\\\\train_sample.csv']\n"
     ]
    }
   ],
   "source": [
    "# vendo os arquivos disponiveis da pasta de dados\n",
    "filenames = glob('data\\*csv')\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "      <td>2017-11-07 09:30:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-07 13:40:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "      <td>2017-11-07 18:05:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>2017-11-07 04:58:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-09 09:00:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time attributed_time  \\\n",
       "0   87540   12       1  13      497 2017-11-07 09:30:38             NaN   \n",
       "1  105560   25       1  17      259 2017-11-07 13:40:27             NaN   \n",
       "2  101424   12       1  19      212 2017-11-07 18:05:24             NaN   \n",
       "3   94584   13       1  13      477 2017-11-07 04:58:08             NaN   \n",
       "4   68413   12       1   1      178 2017-11-09 09:00:09             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lendo o arquivo de sample de treino\n",
    "filename_train_sample = [f for f in filenames if 'train_sample' in f][0]\n",
    "df_train_sample = pd.read_csv(filename_train_sample)\n",
    "\n",
    "# transformando a coluna\n",
    "df_train_sample['click_time'] = pd.to_datetime(df_train_sample.click_time)\n",
    "\n",
    "# visualizando os dados\n",
    "df_train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "      <td>2017-11-07 09:30:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-07 13:40:27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "      <td>2017-11-07 18:05:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>2017-11-07 04:58:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-09 09:00:09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  device  os  channel          click_time  is_attributed\n",
       "0   12       1  13      497 2017-11-07 09:30:38              0\n",
       "1   25       1  17      259 2017-11-07 13:40:27              0\n",
       "2   12       1  19      212 2017-11-07 18:05:24              0\n",
       "3   13       1  13      477 2017-11-07 04:58:08              0\n",
       "4   12       1   1      178 2017-11-09 09:00:09              0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lendo o arquivo de treino\n",
    "filename_train = [f for f in filenames if 'train' in f and 'sample' not in f and 'reduced' not in f][0]\n",
    "\n",
    "# primeiro vamos ler algumas linhas, para poder setar os tipos das variáveis\n",
    "df_train = pd.read_csv(filename_train, nrows = 100)\n",
    "\n",
    "# definindo os tipos de colunas que iremos ler\n",
    "type_dict = {}\n",
    "for col in df_train.columns:\n",
    "    type_dict[col] = np.dtype('int16')\n",
    "    if col == 'ip':\n",
    "        type_dict[col] = np.dtype('int32')\n",
    "    elif col == 'click_time' or col == 'attributed_time':\n",
    "        continue\n",
    "        type_dict[col] = np.datetime64\n",
    "    elif col == 'is_attributed':\n",
    "        type_dict[col] = np.dtype('int8')\n",
    "\n",
    "# definindo as colunas que não leremos\n",
    "usecols = [col for col in list(df_train.columns) if col != 'attributed_time' ]\n",
    "if not use_ip:\n",
    "    usecols = [x for x in usecols if x != 'ip']\n",
    "\n",
    "# reduzindo o dataset\n",
    "if run_function == 1:\n",
    "    red_csv(filename_train, filename_train_reduced, proportion)\n",
    "elif run_function == 2:\n",
    "    red_csv2(filename_train, filename_train_reduced, proportion)\n",
    "elif run_function == 3:\n",
    "    split_train_in_files(filename_train, filename_train_new, filename_test_new, proportion)   \n",
    "\n",
    "\n",
    "# lendo o arquivo\n",
    "if entire_file:\n",
    "    df_train = pd.read_csv(filename_train, dtype=type_dict, parse_dates=['click_time'], usecols=usecols)\n",
    "elif sample_only:\n",
    "    df_train = pd.read_csv(filename_train_sample, dtype=type_dict, parse_dates=['click_time'], usecols=usecols)\n",
    "elif chunkified:\n",
    "    pass\n",
    "else:\n",
    "    df_train = pd.read_csv(filename_train_reduced, dtype=type_dict, parse_dates=['click_time'], usecols=usecols)\n",
    "\n",
    "# visualizando os dados\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>466</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>111</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>328</td>\n",
       "      <td>2017-11-10 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  device  os  channel          click_time\n",
       "0    9       1   3      107 2017-11-10 04:00:00\n",
       "1    9       1   3      466 2017-11-10 04:00:00\n",
       "2   21       1  19      128 2017-11-10 04:00:00\n",
       "3   15       1  13      111 2017-11-10 04:00:00\n",
       "4   12       1  13      328 2017-11-10 04:00:00"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lendo o arquivo de teste\n",
    "filename_test = [f for f in filenames if 'test' in f][0]\n",
    "amostra = pd.read_csv(filename_test, nrows=1)\n",
    "test_cols = [x for x in usecols if y_column not in x]\n",
    "df_test = pd.read_csv(filename_test, dtype=type_dict, parse_dates=['click_time'], usecols=test_cols)\n",
    "\n",
    "# visualizando os dados\n",
    "df_test.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 184447044, 1: 456846}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAFhCAYAAAAoZ4y+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npPD/ALiWs4EZIDpfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 320x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': 706, 'device': 3475, 'os': 800, 'channel': 202, 'click_time': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAADqCAYAAAB5ueWeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 640x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# desbalanço de classes\n",
    "try:\n",
    "    if not dist:\n",
    "        dist = {}\n",
    "        dist[0], dist[1] = 0, 0\n",
    "\n",
    "        unique_dict = {}\n",
    "        for col in df_train.columns:\n",
    "            unique_dict[col] = []\n",
    "\n",
    "        for chunk in tqdm(pd.read_csv(filename_train, usecols=usecols, chunksize=1000000)):\n",
    "            for col in chunk.columns:\n",
    "                if col == y_column:\n",
    "                    dist[0] += chunk[y_column].value_counts()[0]\n",
    "                    dist[1] += chunk[y_column].value_counts()[1]\n",
    "                elif col == 'click_time':\n",
    "                    unique_dict[col] = list(set(unique_dict[col] + [str(t).split()[0] for t in chunk[col].unique()]))\n",
    "                else:\n",
    "                    unique_dict[col] = list(set(unique_dict[col] + list(chunk[col].unique())))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(dist)\n",
    "plt.figure(num=None, figsize=(4, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(dist.keys(), dist.values())\n",
    "\n",
    "plt.title('Distribuição de classes')\n",
    "plt.show()\n",
    "\n",
    "to_plot_dict = {key:len(unique_dict[key]) for key in unique_dict.keys() if key!= y_column}\n",
    "print(to_plot_dict)\n",
    "plt.figure(num=None, figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.bar(to_plot_dict.keys(), to_plot_dict.values())\n",
    "plt.title('Número de valores únicos por coluna')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "code_folding": [
     2,
     8,
     25
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app\n",
      "device\n",
      "os\n",
      "channel\n"
     ]
    }
   ],
   "source": [
    "# pela descrição dos dados, as colunas ip, app, device, os, channel são categóricas\n",
    "# vamos criar colunas dummy para tratar isso\n",
    "if use_ip:\n",
    "    cat_cols = ['ip', 'app', 'device', 'os', 'channel']\n",
    "else:\n",
    "    cat_cols = ['app', 'device', 'os', 'channel']\n",
    "\n",
    "# função que dummifica as colunas\n",
    "def dummify_column(column_name, n, df, least_common = ''):\n",
    "    if df[column_name].value_counts().__len__() > n:\n",
    "        if not least_common:\n",
    "            least_common = list(df[column_name].value_counts().index[n:])\n",
    "        df.loc[df[column_name].isin(least_common), column_name] = -1\n",
    "    tmp_df = pd.get_dummies(df[column_name])\n",
    "    for col in tmp_df.columns:\n",
    "        tmp_df[col] = tmp_df[col].astype(np.dtype('int8'))\n",
    "\n",
    "    tmp_df.columns = [column_name + '_' + str(col) for col in tmp_df.columns]\n",
    "    df = df.merge(tmp_df, how='left', left_index=True, right_index=True)\n",
    "    df.drop(column_name, inplace=True, axis=1)\n",
    "    return df, least_common\n",
    "    print(df.head())\n",
    "\n",
    "least_common_dict = {}\n",
    "# dummificando\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    df_train, least_common_dict[col] = dummify_column(col, 20, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'click_time'\n"
     ]
    }
   ],
   "source": [
    "# função que extrai features dos horarios\n",
    "def extract_time_features(df, column_name):\n",
    "    try:\n",
    "#         df[column_name + '_day_of_year'] = df[column_name].dt.dayofyear.astype(np.int16)\n",
    "#         df[column_name + '_day_of_week'] = df[column_name].dt.dayofweek.astype(np.int8)\n",
    "        df[column_name + '_hour'] = df[column_name].dt.hour.astype(np.int8)\n",
    "#         df[column_name + '_day_of_month'] = df[column_name].dt.day.astype(np.int8)\n",
    "        df.drop(column_name, axis=1, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "if not chunkified:\n",
    "    extract_time_features(df_train, 'click_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# adicionar numero de clicks por ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# vamos por enquanto fazer uma separação simples entre train e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "if chunkified:\n",
    "    X_train, X_test, y_train, y_test = [0,0,0,0]\n",
    "else:\n",
    "    X = df_train[[col for col in df_train.columns if col != y_column]].copy()\n",
    "    y = df_train[y_column].copy()\n",
    "\n",
    "    import gc\n",
    "\n",
    "    # limpando\n",
    "    del df_train; gc.collect()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    del X, y; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "code_folding": [
     2,
     83,
     109,
     112,
     119,
     129
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-a26bd14e7c05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mmax_size_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_size_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;31m# Chamamos a função para criar nosso modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vamos primeiro fazer um entity embedding; podemos usar a saida das camadas de embedding como features pros nossos modelos\n",
    "\n",
    "def create_model(max_ip, max_app, max_device,\n",
    "                 max_os, max_channel, max_hour, lr = 0.0005,\n",
    "                 beta_1 = 0.9, beta_2 = 0.999, activation='relu',\n",
    "                nn_arch = [16,12,4]):\n",
    "    '''\n",
    "        Essa função vai encapsular o processo de criação e definição da nossa rede neural.\n",
    "        Ela espera receber como entrada o maior índice de cada uma das colunas dos nossos\n",
    "        dados de treino.\n",
    "    '''\n",
    "    \n",
    "    # Importando ferramentas do Keras bem úteis\n",
    "    from keras.layers import Embedding, concatenate, Dense, Flatten, Activation, Input\n",
    "    from keras import Model\n",
    "    from keras.optimizers import Adam\n",
    "    \n",
    "    # Cada uma dessas camadas de Input vai apenas marcar o lugar\n",
    "    # de onde vamos colocar cada uma das colunas de dados do df\n",
    "    # de treino.\n",
    "    #ip_input = Input(shape=(1,), dtype='int32')\n",
    "    app_input = Input(shape=(1,), dtype='int16', name='app_input')\n",
    "    device_input = Input(shape=(1,), dtype='int16', name = 'device_input')\n",
    "    os_input = Input(shape=(1,), dtype='int16', name = 'os_input')\n",
    "    channel_input = Input(shape=(1,), dtype='int16', name = 'channel_input')\n",
    "    hour_input = Input(shape=(1,), dtype='int8', name = 'hour_input')\n",
    "    \n",
    "    \n",
    "    # Aqui vamos criar as camadas de embedding, com ela vamos aprender as \n",
    "    # representações das categorias. Essa camada normalmente é utilizada para tratar\n",
    "    # texto então vamos entender os parâmetros. output_dim = 'Numero de neuronias de saida'\n",
    "    # input_dim = 'Tamanho do nosso vocabulário' (no nosso caso, o número de categorias)\n",
    "    # input_length = 'Tamanho da sequência de texto que estamos enviando'\n",
    "    \n",
    "    #ip_out = Embedding(output_dim=16, input_dim=max_ip, input_length=1)(ip_input)\n",
    "    #ip_out = Flatten()(ip_out)\n",
    "    \n",
    "    app_out = Embedding(output_dim=10, input_dim=max_app, input_length=1)(app_input)\n",
    "    app_out = Flatten(name = 'app_out')(app_out)\n",
    "    \n",
    "    device_out = Embedding(output_dim=12, input_dim=max_device, input_length=1)(device_input)\n",
    "    device_out = Flatten(name = 'device_out')(device_out)\n",
    "    \n",
    "    os_out = Embedding(output_dim=10, input_dim=max_os, input_length=1)(os_input)\n",
    "    os_out = Flatten(name = 'os_out')(os_out)\n",
    "    \n",
    "    channel_out = Embedding(output_dim=8, input_dim=max_channel, input_length=1)(channel_input)\n",
    "    channel_out = Flatten(name = 'channel_out')(channel_out)\n",
    "    \n",
    "    hour_out = Embedding(output_dim=6,input_dim=max_hour, input_length=1)(hour_input)\n",
    "    hour_out = Flatten(name = 'hour_out')(hour_out)\n",
    "    \n",
    "    \n",
    "    # Concatenamos as camadas de embedding para aplicarmos outras camadas em cima \n",
    "    # da rede toda.\n",
    "    x = concatenate([app_out,device_out, #ip_out,\n",
    "                    os_out, channel_out,hour_out])\n",
    "    \n",
    "    \n",
    "    # Algumas tradicionais dense layers\n",
    "    for lay in nn_arch:\n",
    "        x = Dense(lay, activation=activation)(x)\n",
    "    \n",
    "    # Layer de saida\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Aqui criamos de fato o modelo, especificando saida e entrada\n",
    "    model = Model(inputs=[ app_input, #ip_input,\n",
    "                         device_input, os_input,\n",
    "                         channel_input, hour_input],\n",
    "                 outputs=[x])\n",
    "    \n",
    "    # Criamos o nosso optimizador o Adam\n",
    "    adam = Adam(beta_1=beta_1, beta_2=beta_2, lr=lr)\n",
    "    \n",
    "    # Compilamos o nosso modelo com a função de custo que queremos otimizar\n",
    "    # e definimos o otimizador a ser utilizado\n",
    "    model.compile(loss='binary_crossentropy',optimizer=adam)\n",
    "    \n",
    "    # Retornamos o modelo\n",
    "    return app_out, device_out, os_out, channel_out, hour_out, model\n",
    "\n",
    "# função que calcula o máximo de cada coluna\n",
    "def calc_size_cols():\n",
    "    '''\n",
    "        Aqui vamos varrer os dados de treino para descobrir\n",
    "        o tamanho de cada categoria.\n",
    "    '''\n",
    "    \n",
    "    # Criamos um dicionario para as saidas, se você não entendeu \n",
    "    max_values = {col : -1 for col in cat_cols}\n",
    "\n",
    "    # Lemos o arquivo em chunks para economizar a memória dos nossos\n",
    "    # computadores.\n",
    "    for chunk in tqdm(pd.read_csv(filename_train_new,chunksize=1000000, dtype=type_dict,usecols=cat_cols)):\n",
    "\n",
    "        # Para cada coluna que desejamos colocar no entity embedding\n",
    "        for col in chunk.columns:\n",
    "            \n",
    "            # Testamos se o maior valor que temos salvo é menor \n",
    "            # do que o máximo que acabamos de ver\n",
    "            if max_values[col] < chunk[col].max():\n",
    "                \n",
    "                # Atualizamos o máximo\n",
    "                max_values[col] = chunk[col].max()\n",
    "                \n",
    "    # Retornamos o dicionário com resultados\n",
    "    return max_values\n",
    "\n",
    "try:\n",
    "    if max_size_dict:\n",
    "        pass\n",
    "except:\n",
    "    max_size_dict = calc_size_cols()\n",
    "\n",
    "if not chunkified:\n",
    "    raise Exception('Not chunkified')\n",
    "\n",
    "# Chamamos a função para criar nosso modelo\n",
    "app_out, device_out, os_out, channel_out, hour_out, model = create_model(max_app=769, #max_values['app'],\n",
    "                    max_channel=501, #max_values['channel'],\n",
    "                    max_device=4228, #max_values['device'],\n",
    "                    max_ip=364779, #max_values['ip'],\n",
    "                    max_os=957,\n",
    "                    max_hour=24,\n",
    "                    lr=0.001,\n",
    "                    nn_arch=[16,12,4]) #max_values['os'])\n",
    "\n",
    "# Para cada passo do processo de treinamento\n",
    "for chunk in tqdm(pd.read_csv(filename_train_new, dtype=type_dict, parse_dates=['click_time'], \n",
    "                                      usecols=usecols, chunksize=1000000)):\n",
    "    \n",
    "    # Lemos o arquivo de treino desejado\n",
    "#     chunk['click_time'] = pd.to_datetime(chunk['click_time'])\n",
    "    # Criamos aqui os dados de entrada do processo de treinamento\n",
    "    extract_time_features(chunk, 'click_time')\n",
    "    cols = [col for col in chunk.columns if col != y_column]\n",
    "    X = [chunk[col] for col in cols]\n",
    "    \n",
    "    \n",
    "    # Criamos os rótulos do treino\n",
    "    y = chunk['is_attributed'].values\n",
    "    \n",
    "    # Ajustamos o modelo em X e y, com um tamanho de batch e número de épocas adequado.\n",
    "    # Separamos 10% dos dados para calcular o log_loss de validação e por fim calculamos\n",
    "    # a métrica auc para os dados de treino e de teste do nosso modelo.\n",
    "    model.fit(x=X, y=y, batch_size=4096, epochs=1,\n",
    "         validation_split=0.1,\n",
    "         callbacks=[roc_callback(test_files=[filename_test_new],\n",
    "                                                     train_chunk_x=X,\n",
    "                                                     train_chunk_y=y,\n",
    "                                                     cols = cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - ETA: 24s - loss: 0.69 - ETA: 5s - loss: 0.6873 - ETA: 2s - loss: 0.681 - ETA: 1s - loss: 0.672 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.649 - 2s 18us/step - loss: 0.6394 - val_loss: 0.5578\n",
      "\n",
      "roc_train: 0.8060\n",
      "roc_test: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a3806ffd0>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui o treinamento do entity embedding no dataframe não chunkificado\n",
    "if chunkified:\n",
    "    raise Exception('Chunkified')\n",
    "app_out2, device_out2, os_out2, channel_out2, hour_out2, model2 = create_model(max_app=769, #max_values['app'],\n",
    "                    max_channel=501, #max_values['channel'],\n",
    "                    max_device=4228, #max_values['device'],\n",
    "                    max_ip=364779, #max_values['ip'],\n",
    "                    max_os=957,\n",
    "                    max_hour=24,\n",
    "                    lr=0.001,\n",
    "                    nn_arch=[16,12,12,12,4]) #max_values['os'])\n",
    "\n",
    "\n",
    "extract_time_features(df_train, 'click_time')\n",
    "cols = [col for col in chunk.columns if col != y_column]\n",
    "X = [df_train[col] for col in cols]\n",
    "\n",
    "\n",
    "# Criamos os rótulos do treino\n",
    "y = df_train[y_column].values\n",
    "\n",
    "# Ajustamos o modelo em X e y, com um tamanho de batch e número de épocas adequado.\n",
    "# Separamos 10% dos dados para calcular o log_loss de validação e por fim calculamos\n",
    "# a métrica auc para os dados de treino e de teste do nosso modelo.\n",
    "model2.fit(x=X, y=y, batch_size=4096, epochs=1,\n",
    "     validation_split=0.1,\n",
    "     callbacks=[roc_callback(test_files=[filename_test_new],\n",
    "                                                 train_chunk_x=X,\n",
    "                                                 train_chunk_y=y,\n",
    "                                                 cols = cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02414391, -0.00944699,  0.00500347,  0.04754439, -0.01142563,\n",
       "         0.02776616, -0.028953  , -0.0207436 ,  0.01881424,  0.01305956]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# como visualizar o output das camadas de embedding\n",
    "from keras.models import Model\n",
    "\n",
    "viz_model_dict = {col:\n",
    " [[m2 for m2 in model2.input if col.replace('click_time_','') in m2.name][0],\n",
    " [m2.output for m2 in model2.layers if col.replace('click_time_','') + '_out' in m2.name][0]] for col in cols}\n",
    "\n",
    "viz_model = {col: Model(inputs=viz_model_dict[col][0], outputs=viz_model_dict[col][1]) for col in cols}\n",
    "\n",
    "viz_model['app'].predict([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.802410326151\n",
      "Test result:  0.618735277837\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar um kNN (não funciona bem com muitas variáveis)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "kn = KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "                         leaf_size=30, p=2, metric='minkowski', metric_params=None, \n",
    "                         n_jobs=1)\n",
    "\n",
    "kn_score_train, kn_score_test = train_and_predict(kn, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Training result: ', kn_score_train)\n",
    "print('Test result: ', kn_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.5\n",
      "Test result:  0.5\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar um regressão logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                        intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \n",
    "                        max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "if not chunkified:\n",
    "    score_train, score_test = train_and_predict(lr, X_train, y_train, X_test, y_test, prob = True)\n",
    "else:\n",
    "    model_list = ensemble_chunks(lr, X_train, y_train, X_test, y_test, prob = True)\n",
    "\n",
    "print('Training result: ', score_train)\n",
    "print('Test result: ', score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00316378\n",
      "Iteration 2, loss = 0.00113443\n",
      "Iteration 3, loss = 0.00112047\n",
      "Iteration 4, loss = 0.00110704\n",
      "Iteration 5, loss = 0.00109538\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training result:  0.905693729316\n",
      "Test result:  0.836192835454\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar também uma rede neural\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hl_tuple = tuple(40 for i in range(4))\n",
    "\n",
    "nn = MLPRegressor(hidden_layer_sizes=hl_tuple, activation='relu', solver='adam', alpha=0.0001, \n",
    "                  batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                  power_t=0.4, max_iter=2000, shuffle=False, random_state=None, \n",
    "                  tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                  early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "nn_score_train, nn_score_test = train_and_predict(nn, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Training result: ', nn_score_train)\n",
    "print('Test result: ', nn_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.927535197428\n",
      "Test result:  0.916958736094\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar também uma regressão por Lasso\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=0.001, fit_intercept=True, normalize=False, \n",
    "              precompute=False, copy_X=True, max_iter=3000, tol=0.0001, \n",
    "              warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\n",
    "lasso_score_train, lasso_score_test = train_and_predict(lasso, X_train, y_train, X_test, y_test, partial=True)\n",
    "\n",
    "print('Training result: ', lasso_score_train)\n",
    "print('Test result: ', lasso_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.954061087485\n",
      "Test result:  0.926634832682\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar também uma regressão por Ridge\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.85, fit_intercept=True, normalize=False, copy_X=True, \n",
    "              max_iter=None, tol=0.0001, solver='auto', random_state=None)\n",
    "\n",
    "ridge_score_train, ridge_score_test = train_and_predict(ridge, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Training result: ', ridge_score_train)\n",
    "print('Test result: ', ridge_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.987066291437\n",
      "Test result:  0.928669647498\n"
     ]
    }
   ],
   "source": [
    "# vamos fazer uma random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=60, criterion='mse', max_depth=5, \n",
    "                           min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                           max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                           min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=4, \n",
    "                           random_state=None, verbose=1, warm_start=False)\n",
    "\n",
    "rf_score_train, rf_score_test = train_and_predict(rf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Training result: ', rf_score_train)\n",
    "print('Test result: ', rf_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rafael\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:  0.991621282751\n",
      "Test result:  0.959412724169\n"
     ]
    }
   ],
   "source": [
    "# vamos fazr um xgboost galera\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 5,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "\n",
    "num_round = 150  # the number of training iterations\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "xgb_preds_train = bst.predict(dtrain)[:, 2]\n",
    "\n",
    "xgb_preds_test = bst.predict(dtest)[:, 2]\n",
    "\n",
    "\n",
    "xgb_score_train = roc_auc_score(y_train, xgb_preds_train)\n",
    "\n",
    "xgb_score_test = roc_auc_score(y_test, xgb_preds_test)\n",
    "\n",
    "\n",
    "print('Training result: ', xgb_score_train)\n",
    "print('Test result: ', xgb_score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x0000021F9B39A710>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\rafael\\anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\core.py\", line 324, in __del__\n",
      "    _check_call(_LIB.XGDMatrixFree(self.handle))\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31420/75000 [===========>..................] - ETA: 3:48:36 - loss: 2.6163e- - ETA: 12:02 - loss: 5.4007e-08 - ETA: 6:02 - loss: 2.9913e-08 - ETA: 4:17 - loss: 0.0032    - ETA: 3:23 - loss: 0.002 - ETA: 2:50 - loss: 0.002 - ETA: 2:28 - loss: 0.004 - ETA: 2:11 - loss: 0.005 - ETA: 2:01 - loss: 0.004 - ETA: 1:52 - loss: 0.004 - ETA: 1:45 - loss: 0.004 - ETA: 1:39 - loss: 0.003 - ETA: 1:34 - loss: 0.004 - ETA: 1:30 - loss: 0.003 - ETA: 1:27 - loss: 0.004 - ETA: 1:23 - loss: 0.005 - ETA: 1:20 - loss: 0.007 - ETA: 1:18 - loss: 0.007 - ETA: 1:16 - loss: 0.006 - ETA: 1:13 - loss: 0.006 - ETA: 1:11 - loss: 0.006 - ETA: 1:10 - loss: 0.005 - ETA: 1:08 - loss: 0.005 - ETA: 1:07 - loss: 0.005 - ETA: 1:05 - loss: 0.005 - ETA: 1:04 - loss: 0.004 - ETA: 1:03 - loss: 0.004 - ETA: 1:02 - loss: 0.004 - ETA: 1:01 - loss: 0.004 - ETA: 1:00 - loss: 0.004 - ETA: 59s - loss: 0.004 - ETA: 59s - loss: 0.00 - ETA: 58s - loss: 0.00 - ETA: 57s - loss: 0.00 - ETA: 57s - loss: 0.00 - ETA: 56s - loss: 0.00 - ETA: 56s - loss: 0.00 - ETA: 55s - loss: 0.00 - ETA: 55s - loss: 0.00 - ETA: 54s - loss: 0.00 - ETA: 54s - loss: 0.00 - ETA: 53s - loss: 0.00 - ETA: 53s - loss: 0.00 - ETA: 53s - loss: 0.00 - ETA: 52s - loss: 0.00 - ETA: 52s - loss: 0.00 - ETA: 52s - loss: 0.00 - ETA: 51s - loss: 0.00 - ETA: 51s - loss: 0.00 - ETA: 51s - loss: 0.00 - ETA: 51s - loss: 0.00 - ETA: 50s - loss: 0.00 - ETA: 50s - loss: 0.00 - ETA: 50s - loss: 0.00 - ETA: 50s - loss: 0.00 - ETA: 49s - loss: 0.00 - ETA: 49s - loss: 0.00 - ETA: 49s - loss: 0.00 - ETA: 49s - loss: 0.00 - ETA: 48s - loss: 0.00 - ETA: 48s - loss: 0.00 - ETA: 48s - loss: 0.00 - ETA: 48s - loss: 0.00 - ETA: 47s - loss: 0.00 - ETA: 47s - loss: 0.00 - ETA: 47s - loss: 0.00 - ETA: 47s - loss: 0.00 - ETA: 47s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 46s - loss: 0.00 - ETA: 45s - loss: 0.00 - ETA: 45s - loss: 0.00 - ETA: 45s - loss: 0.00 - ETA: 45s - loss: 0.00 - ETA: 45s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 44s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 43s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 42s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 41s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 40s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 39s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 38s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 37s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 36s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 35s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 34s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 33s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 32s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 31s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 30s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 29s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 28s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 27s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 26s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 25s - loss: 0.00 - ETA: 24s - loss: 0.0064030/75000 [========================>.....] - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 24s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 23s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 22s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 21s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 20s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 19s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 18s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 17s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 16s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 15s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 14s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 13s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 12s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 11s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 10s - loss: 0.00 - ETA: 9s - loss: 0.0020 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.001 - ETA: 9s - loss: 0.001 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 9s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 8s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002075000/75000 [==============================] - ETA: 6s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.001 - ETA: 5s - loss: 0.001 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 41s 552us/step - loss: 0.0021\n",
      "75000/75000 [==============================] - ETA: 58:1 - ETA: 1:2 - ETA: 51s - ETA: 39 - ETA: 33 - ETA: 29 - ETA: 27 - ETA: 25 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 13s 177us/step\n",
      "25000/25000 [==============================] - ETA: 35 - ETA: 7 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 193us/step\n",
      "Training result:  0.5\n",
      "Test result:  0.5\n"
     ]
    }
   ],
   "source": [
    "# vamos tentar uma rede diferente\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1]/2), input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(80, kernel_initializer='normal', activation='relu'))\n",
    "    for i in range(10):\n",
    "        model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=1)\n",
    "\n",
    "# kfold = KFold(n_splits=10, random_state=seed)\n",
    "# results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "\n",
    "# print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "keras_score_train, keras_score_test = train_and_predict(estimator, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Training result: ', keras_score_train)\n",
    "print('Test result: ', keras_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging no treinamento:  0.987164278246\n",
      "Bagging no teste:  0.933756684539\n",
      "Bagging com pesos no treinamento:  0.5\n",
      "Bagging com pesos no teste:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Bagging de modelos\n",
    "model_dict = {\n",
    "#     'lr':lr,\n",
    "#     'lasso':lasso,\n",
    "#     'ridge':ridge,\n",
    "#     'nn':nn,\n",
    "    'rf':rf,\n",
    "    'xgb':bst,\n",
    "#     'keras':estimator\n",
    "}\n",
    "\n",
    "# vamos fazer um bagging dos modelos anteriores\n",
    "def make_prediction_df(model_dict, X, d):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def get_train_test_predictions(X, model):\n",
    "        # função que faz as predictions de um determinado modelo\n",
    "        predictions = model.predict(X)\n",
    "        try:\n",
    "            if predictions.shape[1] > 1:\n",
    "                predictions = predictions[:,2]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    for model in model_dict.keys():\n",
    "        # fazendo as colunas dos dataframes\n",
    "        if model == 'xgb':\n",
    "            df[model] = get_train_test_predictions(d, model_dict[model])\n",
    "        else:\n",
    "            df[model] = get_train_test_predictions(X, model_dict[model])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# rodando a função      \n",
    "bag_df_train = make_prediction_df(model_dict, X_train, dtrain)\n",
    "bag_df_test = make_prediction_df(model_dict, X_test, dtest)\n",
    "\n",
    "# vamos fazer uma média simples primeiro\n",
    "bag_train_score = roc_auc_score(y_train, bag_df_train.mean(axis=1)*10)\n",
    "print('Bagging no treinamento: ', bag_train_score)\n",
    "\n",
    "bag_test_score = roc_auc_score(y_test, bag_df_test.mean(axis=1)-0.01)\n",
    "print('Bagging no teste: ', bag_test_score)\n",
    "\n",
    "# vamos definir o peso de cada modelo por uma regressão\n",
    "bag_lasso = Lasso()\n",
    "\n",
    "bag_lasso_score_train, bag_lasso_score_test = train_and_predict(bag_lasso, bag_df_train, y_train, bag_df_test, y_test)\n",
    "print('Bagging com pesos no treinamento: ', bag_lasso_score_train)\n",
    "print('Bagging com pesos no teste: ', bag_lasso_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "click_id       int64\n",
       "ip             int64\n",
       "app            int64\n",
       "device         int64\n",
       "os             int64\n",
       "channel        int64\n",
       "click_time    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-462872e36475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummify_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleast_common_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprepare_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-a917c50c35a0>\u001b[0m in \u001b[0;36mdummify_column\u001b[1;34m(column_name, n, df, least_common)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mleast_common\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleast_common\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtmp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafael\\anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na,\n\u001b[1;32m-> 1215\u001b[1;33m                                  sparse=sparse, drop_first=drop_first)\n\u001b[0m\u001b[0;32m   1216\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafael\\anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m         \u001b[0mdummy_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# processando o df_test da mesma maneira que o df_train\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    df_test, a = dummify_column(col, 20, df_test, least_common_dict[col])\n",
    "\n",
    "def prepare_submission(X, d, model_dict):\n",
    "    output_df = make_prediction_df(model_dict, X, d)\n",
    "    submission = pd.DataFrame()\n",
    "    submission['is_attributed'] = output_df.mean(axis=1)\n",
    "    submission.reset_index('click_id')\n",
    "    return submission\n",
    "\n",
    "d_submission = xgb.DMatrix(df_test)\n",
    "\n",
    "pred_submission = prepare_submission(df_test, d, model_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
